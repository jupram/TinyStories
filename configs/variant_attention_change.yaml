run_name: variant_attention_change
output_dir: runs
seed: 42
device: auto
precision: bf16
torch_compile: false
use_flash: true
deterministic: true

model:
  name: variant_attention_change
  d_model: 512
  n_layers: 8
  n_heads: 8
  n_kv_heads: 2  # grouped-query attention (4 query heads share one kv head)
  mlp_ratio: 4.0
  dropout: 0.0
  attn_dropout: 0.0
  rope_base: 10000
  seq_len: 256
  resid_scale: 1.0

data:
  dataset_name: roneneldan/TinyStories
  tokenizer_name: gpt2
  seq_len: 256
  num_workers: 2
  val_split: validation
  val_fraction: 0.01
  cache_dir: data/cache
  dataset_variant: full  # options: full, small
  small_train_samples: 50000
  small_val_samples: 5000

optimizer:
  lr: 0.0003
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1.0e-08

scheduler:
  warmup_steps: 200
  min_lr_ratio: 0.1

batch_size: 32
val_batch_size: 32
grad_accum_steps: 1
max_steps: 4000
max_eval_batches: 50
log_every: 50
eval_every: 200
save_every: 1000
clip_grad: 1.0
