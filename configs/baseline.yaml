run_name: baseline
output_dir: runs
seed: 42
device: auto
precision: bf16
torch_compile: false
use_flash: true
deterministic: true

model:
  name: baseline
  d_model: 512
  n_layers: 8
  n_heads: 8
  mlp_ratio: 4.0
  dropout: 0.0
  attn_dropout: 0.0
  rope_base: 10000
  seq_len: 256

data:
  dataset_name: roneneldan/TinyStories
  tokenizer_name: gpt2
  seq_len: 256
  num_workers: 2
  val_split: validation
  val_fraction: 0.01
  cache_dir: data/cache
  dataset_variant: small  # options: full, small
  small_train_samples: 50000
  small_val_samples: 5000

optimizer:
  lr: 0.0003
  weight_decay: 0.1
  betas: [0.9, 0.95]
  eps: 1.0e-08

scheduler:
  warmup_steps: 200
  min_lr_ratio: 0.1

batch_size: 32
val_batch_size: 32
grad_accum_steps: 1
max_steps: 8000
# ~2000 steps * batch_size 32 -> ~64k sequences; this is a partial epoch, not the full dataset
# For ~1 full pass, set max_steps to about ceil(len(train_ds)/batch_size) (~15k when TinyStories is packed at seq_len 256).
# Increase further for multi-epoch training or lower batch_size if you want more optimizer updates with the same token budget.
max_eval_batches: 50
log_every: 50
eval_every: 1000
save_every: 4000
clip_grad: 1.0
